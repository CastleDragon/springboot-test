<?xml version="1.0" encoding="UTF-8"?>
<!-- 每隔x分钟扫描配置 -->
<configuration debug="false" scan="false" scanPeriod="1 minutes">

    <property name="LOG_HOME" value="C:\a" />
    <property name="KAFKA_HOME" value="./kafka-data" />
    <!-- 控制台输出级别 -->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{yyyy-MM-dd HH:mm:ss} %highlight([%-4p] %C{1.}-%M:%L) - %msg%n
            </Pattern>
        </layout>
    </appender>

    <!-- 输出到文件：只记录info级别的log-->
    <appender name="FILE-INFO" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 最新日志记录地址 -->
        <file>${LOG_HOME}/info.log</file>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>INFO</level>
            <!-- 匹配成功处理方式 : ACCEPT，日志会被立即处理，不再经过剩余过滤器 -->
            <onMatch>ACCEPT</onMatch>
            <!-- 匹配不成功处理方式 : DENY，日志将立即被抛弃不再经过其他过滤器-->
            <onMismatch>DENY</onMismatch>
        </filter>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <Pattern>
                %d{yyyy-MM-dd HH:mm:ss} [%-4p] %C{1.}-%M:%L - %msg%n
            </Pattern>
        </encoder>
        <!-- 基于时间的分包策略，分包间隔是根据fileNamePattern中指定的事件最小单位，比如例子中的%d{yyyy-MM-dd_HH-mm}的最小事件单位为分，它的触发方式就是1分钟 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_HOME}/archived/info.%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <!-- 策略还可以互相嵌套，比如本例中在时间策略中又嵌套了文件大小策略,日志文件超过大小限制后,备份归档地址已经log文件格式 -->
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>10MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
    </appender>

    <!-- 输出到文件：只记录error级别的log-->
    <appender name="FILE-ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 最新日志记录地址 -->
        <file>${LOG_HOME}/error.log</file>
        <filter class="ch.qos.logback.classic.filter.LevelFilter"><!-- 只打印错误日志 -->
            <level>ERROR</level>
            <!-- 匹配成功处理方式 : ACCEPT，日志会被立即处理，不再经过剩余过滤器 -->
            <onMatch>ACCEPT</onMatch>
            <!-- 匹配不成功处理方式 : DENY，日志将立即被抛弃不再经过其他过滤器-->
            <onMismatch>DENY</onMismatch>
        </filter>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <Pattern>
                %d{yyyy-MM-dd HH:mm:ss} [%-5p] %C{1.}-%M:%L - %msg%n
            </Pattern>
        </encoder>
        <!-- 基于时间的分包策略，分包间隔是根据fileNamePattern中指定的事件最小单位，比如例子中的%d{yyyy-MM-dd_HH-mm}的最小事件单位为分，它的触发方式就是1分钟 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_HOME}/archived/error.%d{yyyy-MM-dd_HH}.%i.log</fileNamePattern>
            <!-- 策略还可以互相嵌套，比如本例中在时间策略中又嵌套了文件大小策略,日志文件超过大小限制后,备份归档地址已经log文件格式 -->
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>10MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
    </appender>

    <!-- 特别注意：这个目录类中的 *Listener 类只能用来打印 kafka 收到的数据,
        会写到指定的文件, 这个文件是需要定时上传到ftp
        所以一定不要再这个目录中的 *Listener 类写其他日志
     -->


    <root level="info">
        <appender-ref ref="CONSOLE" />
    <appender-ref ref="FILE-INFO" />
</root>
</configuration>